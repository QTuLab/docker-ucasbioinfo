{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28460aac-1e1d-4d94-a40d-812e41520072",
   "metadata": {},
   "source": [
    "# SCENIC+ 分析流程：从 scRNA/scATAC 数据到增强子调控网络 (eGRN)\n",
    "\n",
    "## 概述\n",
    "\n",
    "本 Jupyter Notebook 是一个完整的操作指南，旨在演示如何运用 **SCENIC+**  工具包，从独立的单细胞 RNA 测序 (scRNA-seq) 和单细胞 ATAC 测序 (scATAC-seq) 数据出发，一步步构建并推断出**增强子驱动的基因调控网络 (enhancer-driven Gene Regulatory Network, eGRN)** 。\n",
    "\n",
    "为了让这个复杂的多组学分析流程稳健、可复现且易于理解，我们在设计中融入了以下核心特性：\n",
    "\n",
    "1. 模块化设计 (Modular Design)  \n",
    "   我们将整个分析流程分解为多个逻辑清晰的模块（如：scRNA 数据预处理、scATAC 数据处理、GRN 推断、下游可视化等）。这种设计不仅让每一步的目标更明确，也方便你聚焦特定环节，轻松定位和解决问题。\n",
    "\n",
    "2. 断点续跑机制 (Checkpointing)  \n",
    "   对于耗时较长的计算步骤，我们都设置了断点续跑机制。代码在执行前会检查结果文件是否存在。如果存在，则直接加载结果并跳过计算，极大地节省了重复运行的时间。这一设计让你可以随时中断、调整参数或从错误中恢复，而无需从头来过。\n",
    "\n",
    "3. Snakemake 集成指引 (Snakemake Integration)  \n",
    "   本 Notebook 是 SCENIC+ Snakemake 核心流程的启动器和结果浏览器。你将在 Notebook 的引导下准备输入文件、生成配置文件，并获取在终端中启动 Snakemake 分析的指令。计算完成后，再回到 Notebook 中加载并探索最终的 eGRN 结果。\n",
    "\n",
    "## 快速开始\n",
    "\n",
    "### 第 1 步：准备环境\n",
    "\n",
    "在开始之前，请确保你已成功构建并运行本项目提供的 Docker 镜像。该镜像包含了所有必需的软件依赖和环境配置。\n",
    "\n",
    "### 第 2 步：选择你的分析路径并准备数据\n",
    "\n",
    "本教程设计了三条进阶路径，你可以根据自己的计算资源和学习目标选择从哪里开始。\n",
    "\n",
    "**路径一：探索和可视化**\n",
    ">\n",
    "> - **目标**: 跳过所有计算步骤，直接学习如何解读和可视化 eGRN 结果。\n",
    "> - **适用人群**: 希望快速了解项目产出，或计算资源受限的用户。\n",
    "> - **资源需求**: 任何配置的电脑，运行仅需 **2-5 分钟**。\n",
    "> - **所需数据**:\n",
    ">\n",
    ">   - **[ucasbioinfo_scenic_output.tar.gz (2.4 GB)](https://tulab.genetics.ac.cn/~qtu/ucasbioinfo/ucasbioinfo_scenic_output.tar.gz)** : 双组学分析部分的输出结果。解压后置于 `output/` 目录。 \n",
    ">   - **[ucasbioinfo_scenic_pipeline.tar.gz (2.4 GB)](https//tulab.genetics.ac.cn/~qtu/ucasbioinfo/ucasbioinfo_scenic_pipeline.tar.gz)** : SCENIC+ 分析流程的输出结果。解压后置于 `scplus_pipeline/` 目录。\n",
    "\n",
    "\n",
    "**路径二：双组学数据分析**\n",
    ">\n",
    "> - **目标**: 完成 scRNA-seq 和 scATAC-seq 的质控、降维、聚类等初步分析。\n",
    "> - **适用人群**: 适合初学者，或希望在个人电脑上完成大部分分析的用户。\n",
    "> - **资源需求**: 普通个人电脑，运行需要 **30-60 分钟**。\n",
    "> - **所需数据**:\n",
    ">\n",
    ">   - **[ucasbioinfo_scenic_input.tar.gz (1.8 GB)](https://tulab.genetics.ac.cn/~qtu/ucasbioinfo/ucasbioinfo_scenic_input.tar.gz)** : 下载并解压到项目根目录下的 `input/` 文件夹。\n",
    "\n",
    "**路径三：完整的 eGRN 推断**\n",
    ">\n",
    "> - **目标**: 运行资源消耗巨大的 SCENIC+ Snakemake 流程，推断完整的 eGRN。\n",
    "> - **适用人群**: 拥有服务器或高性能计算资源的用户。\n",
    "> - **资源需求**: 高性能计算环境 (建议 **64 GB+ 内存**)，预计耗时 **5-10 小时**。\n",
    "> - **所需数据**:\n",
    ">\n",
    ">   - 完成 **路径二** 的所有步骤和数据准备。\n",
    ">   - **cisTarget 数据库**: 从 [Aertslab 官网](https://resources.aertslab.org/cistarget/databases/ \"null\") 下载人类 (hg38) 的数据库文件（[`...rankings.feather`](https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/screen/mc_v10_clust/region_based/hg38_screen_v10_clust.regions_vs_motifs.rankings.feather) (33G) 和 [`...scores.feather`](https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/screen/mc_v10_clust/region_based/hg38_screen_v10_clust.regions_vs_motifs.scores.feather) (13G)），同样放入 `input/` 目录。这是运行 Snakemake 流程必需的，文件体积巨大，计算密集。\n",
    "\n",
    "### 第 3 步：运行 Notebook\n",
    "\n",
    "1. **顺序执行单元格**：请从头开始，严格按照 Notebook 的单元格顺序执行代码，不要跳步。\n",
    "2. **根据路径执行**：根据你在第 2 步选择的路径，你可以：\n",
    "\n",
    "    - 完成 **路径一** 的步骤后停止。此时你将得到一个完整的双组学分析案例。\n",
    "    - 在路径一的基础上，继续执行并启动 Snakemake 流程以完成 **路径二**。\n",
    "    - 如果选择 **路径三**，则加载所有预计算结果，直接跳转到 Notebook 的后半部分进行探索。\n",
    "3. **探索结果**：当计算（或加载）完成后，即可使用 Notebook 后续的单元格来分析和可视化最终结果。\n",
    "\n",
    "## 参考资料\n",
    "\n",
    "在学习过程中，如果对具体的分析工具或步骤有疑问，建议查阅以下官方文档：\n",
    "\n",
    "- [Scanpy 官方教程](https://scanpy.readthedocs.io/en/stable/tutorials/index.html)\n",
    "- [pycisTopic 官方教程](https://pycistopic.readthedocs.io/en/latest/tutorials.html)\n",
    "- [SCENIC+ 官方教程](https://scenicplus.readthedocs.io/en/latest/tutorials.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618a8f6a",
   "metadata": {},
   "source": [
    "---\n",
    "## **0. 环境设置与包加载**\n",
    "本单元格加载所有需要的Python库，并设置一些基础绘图参数。每次启动内核后，这是第一个需要运行的单元格。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac5e5d2-2e9e-4dcb-8ffd-e5864e3b69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 忽略报警信息\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    # message=\"pkg_resources is deprecated as an API\",\n",
    "    category=UserWarning\n",
    ")\n",
    "warnings.filterwarnings(\n",
    "    'ignore',\n",
    "    category=FutureWarning\n",
    ")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d37de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "import pickle\n",
    "import yaml\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scanpy etc\n",
    "import anndata\n",
    "import scanpy as sc\n",
    "import scrublet as scr\n",
    "\n",
    "# pycisTopic and scenicplus\n",
    "import scenicplus\n",
    "import pycisTopic\n",
    "import pyranges as pr\n",
    "\n",
    "from pycisTopic.cistopic_class import create_cistopic_object_from_fragments\n",
    "from pycisTopic.clust_vis import (find_clusters, run_umap, run_tsne, plot_metadata, plot_imputed_features, plot_topic, cell_topic_heatmap)\n",
    "from pycisTopic.diff_features import impute_accessibility, find_diff_features\n",
    "from pycisTopic.gene_activity import get_gene_activity\n",
    "from pycisTopic.iterative_peak_calling import get_consensus_peaks\n",
    "from pycisTopic.lda_models import run_cgs_models_mallet, evaluate_models\n",
    "from pycisTopic.plotting.qc_plot import plot_sample_stats, plot_barcode_stats\n",
    "from pycisTopic.pseudobulk_peak_calling import export_pseudobulk, peak_calling\n",
    "from pycisTopic.qc import get_barcodes_passing_qc_for_sample\n",
    "from pycisTopic.topic_binarization import binarize_topics\n",
    "from pycisTopic.topic_qc import compute_topic_metrics, plot_topic_qc, topic_annotation\n",
    "from pycisTopic.utils import region_names_to_coordinates\n",
    "\n",
    "# 打印版本号以确保环境一致性\n",
    "print(f\"scanpy version: {sc.__version__}\")\n",
    "print(f\"pycisTopic version: {pycisTopic.__version__}\")\n",
    "print(f\"scenicplus version: {scenicplus.__version__}\")\n",
    "\n",
    "# 定义全局常量\n",
    "# 使用 os.path.join 确保跨平台兼容性\n",
    "WORK_DIR = os.getcwd()\n",
    "OUT_DIR = os.path.join(WORK_DIR, \"output\")\n",
    "IN_DIR = os.path.join(WORK_DIR, \"input\")\n",
    "SCPLUS_DIR = os.path.join(WORK_DIR, \"scplus_pipeline\")\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "os.makedirs(IN_DIR, exist_ok=True)\n",
    "os.makedirs(SCPLUS_DIR, exist_ok=True)\n",
    "\n",
    "# 默认使用的CPU数量\n",
    "N_CPU = 4\n",
    "\n",
    "# 定义一个任务完成的提醒音\n",
    "from IPython.display import Audio\n",
    "def notify_chord(duration_per_note=0.15, freqs=[523.25, 659.25, 783.99]):\n",
    "    \"\"\"\n",
    "    播放一个柔和的、上升的和弦音。\n",
    "    freqs: 一个包含多个频率的列表，代表和弦的音符。\n",
    "    \"\"\"\n",
    "    framerate = 44100\n",
    "    \n",
    "    # 为每个音符生成音频数据\n",
    "    audio_segments = []\n",
    "    for freq in freqs:\n",
    "        t = np.linspace(0., duration_per_note, int(framerate * duration_per_note))\n",
    "        # 使用一个衰减包络让声音更柔和\n",
    "        envelope = np.exp(-np.linspace(0, 5, len(t)))\n",
    "        audio_segment = 0.5 * np.sin(2. * np.pi * freq * t) * envelope\n",
    "        audio_segments.append(audio_segment)\n",
    "        # 音符间的短暂静音\n",
    "        audio_segments.append(np.zeros(int(framerate * 0.05)))\n",
    "        \n",
    "    # 合并所有音频片段\n",
    "    audio_data = np.concatenate(audio_segments)\n",
    "    \n",
    "    return Audio(audio_data, rate=framerate, autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12371fd4",
   "metadata": {},
   "source": [
    "---\n",
    "## **第一部分：scRNA-seq 数据分析**\n",
    "\n",
    "本部分处理scRNA-seq数据，从读取、质控到降维聚类，最终保存处理好的 AnnData 对象作为检查点。这部分速度很快。\n",
    "\n",
    "### **1.1. 加载和处理 scRNA-seq 数据**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42dbf0c-4aeb-47bc-8fe5-343604f948f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义相关路径\n",
    "adata_path = os.path.join(IN_DIR, 'filtered_feature_bc_matrix')\n",
    "cell_metadata_path = os.path.join(IN_DIR, 'cell_data.tsv')\n",
    "final_adata_path = os.path.join(OUT_DIR, 'scRNA.h5ad')\n",
    "\n",
    "# 检查点：如果处理好的 AnnData 文件已存在，则直接加载\n",
    "if os.path.exists(final_adata_path):\n",
    "    print(f\"INFO: Found processed scRNA-seq data at {final_adata_path}. Loading from file.\")\n",
    "    adata = sc.read_h5ad(final_adata_path)\n",
    "else:\n",
    "    print(\"INFO: Processed scRNA-seq data not found. Starting from raw data. This will take time.\")\n",
    "    \n",
    "    # 1. 读取10x数据\n",
    "    adata = sc.read_10x_mtx(adata_path, var_names=\"gene_symbols\")\n",
    "    adata.var_names_make_unique()\n",
    "\n",
    "    # 2. 合并细胞注释信息\n",
    "    cell_data = pd.read_table(cell_metadata_path, index_col=0)\n",
    "    cell_data.index = [cb.rsplit(\"-\", 1)[0] for cb in cell_data.index]\n",
    "    \n",
    "    # 寻找并保留共同的细胞\n",
    "    common_cells = list(set(adata.obs_names) & set(cell_data.index))\n",
    "    adata = adata[common_cells, :].copy()\n",
    "    adata.obs = cell_data.loc[adata.obs_names, :]\n",
    "\n",
    "    # 3. 标准化和筛选高变基因 (注意：QC计算移到下一个单元格)\n",
    "    adata.raw = adata\n",
    "    sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "    sc.pp.log1p(adata)\n",
    "    sc.pp.highly_variable_genes(adata, min_mean=0.0125, max_mean=3, min_disp=0.5)\n",
    "    # 注：我们暂时不进行 highly_variable gene 的筛选，以便在 QC 中看到所有基因的情况\n",
    "    \n",
    "    # 4. 保存处理好的对象\n",
    "    print(f\"INFO: Saving processed scRNA-seq data to {final_adata_path}\")\n",
    "    adata.write_h5ad(final_adata_path)\n",
    "\n",
    "print(f\"\\nOK: `adata` object is loaded with {adata.n_obs} cells and {adata.n_vars} genes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5714516d",
   "metadata": {},
   "source": [
    "**质量控制 (QC) 与可视化**\n",
    "\n",
    "- 常见质控参数：\n",
    "  - `min_genes=200`：该参数用于过滤细胞。它将过滤掉表达的基因总数少于200个的细胞。这些细胞通常被认为是低质量的、死的或空的液滴。\n",
    "  - `min_cells=3`：该参数用于过滤基因。它将过滤掉在少于3个细胞中表达的基因。这些基因通常是稀有基因或噪音，对下游分析贡献不大。\n",
    "  - 线粒体基因：此处使用 `MT-` 前缀来识别线粒体基因，这是人类数据的标准。对于小鼠数据，其前缀通常是小写的 `mt-`。\n",
    "- 可以根据初步的QC图表决定具体参数，然后重新过滤并画图，确保过滤后的结果符合预期。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5cf988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查 QC 指标是否已计算\n",
    "if 'n_genes_by_counts' not in adata.obs.columns:\n",
    "    print(\"INFO: QC metrics not found. Calculating now...\")\n",
    "    adata.var[\"mt\"] = adata.var_names.str.startswith(\"MT-\")\n",
    "    sc.pp.calculate_qc_metrics(adata, qc_vars=[\"mt\"], percent_top=None, log1p=False, inplace=True)\n",
    "    print(\"OK: QC metrics calculated.\")\n",
    "else:\n",
    "    print(\"INFO: QC metrics already exist.\")\n",
    "\n",
    "# 打印关键 QC 指标的统计信息\n",
    "print(\"\\n--- QC Summary ---\")\n",
    "print(adata.obs[['n_genes_by_counts', 'total_counts', 'pct_counts_mt']].describe())\n",
    "\n",
    "# --- 逐个绘制小提琴图 ---\n",
    "print(\"\\n--- QC Violin Plots ---\")\n",
    "\n",
    "qc_metrics = ['n_genes_by_counts', 'total_counts', 'pct_counts_mt']\n",
    "\n",
    "fig, axes = plt.subplots(1, len(qc_metrics), figsize=(15, 5))\n",
    "for i, metric in enumerate(qc_metrics):\n",
    "    sc.pl.violin(\n",
    "        adata,\n",
    "        keys=[metric],\n",
    "        jitter=0.4,\n",
    "        ax=axes[i],\n",
    "        show=False\n",
    "    )\n",
    "    axes[i].set_title(metric)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# (可选的) 细胞筛选步骤\n",
    "# 基于上面的图和统计数据，你可以决定筛选阈值\n",
    "# n_genes_min = 200\n",
    "# n_genes_max = 2500\n",
    "# pct_mt_max = 5\n",
    "# print(f\"\\nINFO: Before filtering: {adata.n_obs} cells\")\n",
    "# sc.pp.filter_cells(adata, min_genes=n_genes_min)\n",
    "# adata = adata[adata.obs.n_genes_by_counts < n_genes_max, :].copy()\n",
    "# adata = adata[adata.obs.pct_counts_mt < pct_mt_max, :].copy()\n",
    "# print(f\"INFO: After filtering: {adata.n_obs} cells remain.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58b7770",
   "metadata": {},
   "source": [
    "**降维与 UMAP 可视化**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba1365-98ff-4e1f-a01f-5660f1ee770d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 筛选高变基因 (在最终的细胞集上进行)\n",
    "print(\"INFO: Finding highly variable genes...\")\n",
    "# 使用 'cell_ranger' 风格，它更稳健且不依赖于 log1p 的特定元数据\n",
    "# sc.pp.highly_variable_genes(adata, flavor='cell_ranger', n_top_genes=4000)\n",
    "# 或者尝试 'seurat_v3'，它通常也更推荐\n",
    "sc.pp.highly_variable_genes(adata, flavor='seurat_v3', n_top_genes=4000)\n",
    "adata_hvg = adata[:, adata.var.highly_variable].copy()\n",
    "print(f\"Found {adata_hvg.n_vars} highly variable genes for downstream analysis.\")\n",
    "\n",
    "\n",
    "# 标准化、降维和聚类\n",
    "print(\"INFO: Scaling, running PCA, neighbors, and UMAP...\")\n",
    "sc.pp.scale(adata_hvg, max_value=10)\n",
    "sc.tl.pca(adata_hvg)\n",
    "sc.pp.neighbors(adata_hvg)\n",
    "sc.tl.umap(adata_hvg)\n",
    "\n",
    "# 可视化最终的 UMAP 结果\n",
    "print(\"\\n--- Final UMAP Visualization ---\")\n",
    "sc.pl.umap(adata_hvg, color=\"Seurat_cell_type\", title=\"scRNA-seq UMAP\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a9fbdb6",
   "metadata": {},
   "source": [
    "---\n",
    "## **第二部分：scATAC-seq 数据分析 (pycisTopic)**\n",
    "\n",
    "这部分处理 scATAC-seq 数据，包括产生 peak calling, concensus peak, QC 等步骤。比较耗时（1-2小时），相应步骤都设计为可断点续跑，有示范数据的话可以直接加载进行可视化。\n",
    "\n",
    "### **2.1. ATAC-seq 设置与文件路径定义**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc0da01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义输入文件路径\n",
    "fragments_file = os.path.join(IN_DIR, \"fragments.tsv.gz\")\n",
    "chromsizes_file = os.path.join(IN_DIR, \"hg38.chrom.sizes\")\n",
    "blacklist_file = os.path.join(IN_DIR, \"hg38-blacklist.v2.bed\")\n",
    "\n",
    "# 假设数据已下载\n",
    "# !wget -O {fragments_file} https://cf.10xgenomics.com/samples/cell-arc/1.0.0/human_brain_3k/human_brain_3k_atac_fragments.tsv.gz\n",
    "# !wget -O {fragments_file}.tbi https://cf.10xgenomics.com/samples/cell-arc/1.0.0/human_brain_3k/human_brain_3k_atac_fragments.tsv.gz.tbi\n",
    "\n",
    "fragments_dict = {\"10x_multiome_brain\": fragments_file}\n",
    "\n",
    "# 读取染色体长度\n",
    "chromsizes = pd.read_table(chromsizes_file, header=None, names=[\"Chromosome\", \"End\"])\n",
    "chromsizes.insert(1, \"Start\", 0)\n",
    "\n",
    "# 定义所有输出路径\n",
    "consensus_peak_dir = os.path.join(OUT_DIR, \"consensus_peak_calling\")\n",
    "bed_path = os.path.join(consensus_peak_dir, \"pseudobulk_bed_files\")\n",
    "bigwig_path = os.path.join(consensus_peak_dir, \"pseudobulk_bw_files\")\n",
    "macs_outdir = os.path.join(consensus_peak_dir, \"MACS\")\n",
    "consensus_bed_path = os.path.join(consensus_peak_dir, \"consensus_regions.bed\")\n",
    "qc_dir = os.path.join(OUT_DIR, \"qc\")\n",
    "tss_bed_path = os.path.join(qc_dir, \"tss.bed\")\n",
    "\n",
    "# 确保所有输出目录都存在\n",
    "for path in [bed_path, bigwig_path, macs_outdir, qc_dir]:\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "print(\"INFO: ATAC-seq paths defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47d3a03",
   "metadata": {},
   "source": [
    "### **2.2. [耗时] 导出 Pseudobulk 文件**\n",
    "此步骤为每个细胞类型生成聚合的BED和BigWig文件，用于后续的peak calling。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9696161c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点：检查输出目录是否已经有文件\n",
    "if os.path.exists(bed_path) and len(os.listdir(bed_path)) > 0:\n",
    "    print(\"INFO: Pseudobulk files found. Skipping computation and reconstructing paths.\")\n",
    "    bed_files = glob.glob(os.path.join(bed_path, \"*.bed.gz\"))\n",
    "    bed_paths = {os.path.basename(f).replace(\".bed.gz\", \"\"): f for f in bed_files}\n",
    "    bw_files = glob.glob(os.path.join(bigwig_path, \"*.bw\"))\n",
    "    bw_paths = {os.path.basename(f).replace(\".bw\", \"\"): f for f in bw_files}\n",
    "    print(f\"OK: Reconstructed {len(bed_paths)} BED and {len(bw_paths)} BigWig paths.\")\n",
    "else:\n",
    "    print(\"INFO: Pseudobulk files not found. Running export_pseudobulk.\")\n",
    "    bw_paths, bed_paths = export_pseudobulk(\n",
    "        input_data=adata.obs, # 使用adata.obs中的注释\n",
    "        variable=\"VSN_cell_type\",\n",
    "        sample_id_col=\"VSN_sample_id\",\n",
    "        chromsizes=chromsizes,\n",
    "        bed_path=bed_path,\n",
    "        bigwig_path=bigwig_path,\n",
    "        path_to_fragments=fragments_dict,\n",
    "        n_cpu=N_CPU,\n",
    "        normalize_bigwig=True,\n",
    "        temp_dir=\"/tmp\",\n",
    "        split_pattern=\"-\"\n",
    "    )\n",
    "    print(\"OK: export_pseudobulk finished.\")\n",
    "    notify_chord()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8805c371",
   "metadata": {},
   "source": [
    "### **2.3. [耗时] 使用 MACS2 进行 Peak Calling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e56bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "macs_path = shutil.which('macs2')\n",
    "if macs_path is None:\n",
    "    raise FileNotFoundError(\"MACS2 not found in system PATH.\")\n",
    "\n",
    "# 检查点：检查是否已生成所有预期的narrowPeak文件\n",
    "expected_peak_files_count = len(bed_paths)\n",
    "existing_peak_files = glob.glob(os.path.join(macs_outdir, '*_peaks.narrowPeak'))\n",
    "\n",
    "if len(existing_peak_files) >= expected_peak_files_count:\n",
    "    print(f\"INFO: Found {len(existing_peak_files)} MACS2 narrowPeak files. Skipping peak_calling.\")\n",
    "    narrow_peak_dict = {\n",
    "        os.path.basename(f).replace('_peaks.narrowPeak', ''): f for f in existing_peak_files\n",
    "    }\n",
    "else:\n",
    "    print(f\"INFO: Expected {expected_peak_files_count} peak files, found {len(existing_peak_files)}. Running MACS2.\")\n",
    "    narrow_peak_dict = peak_calling(\n",
    "        macs_path=macs_path,\n",
    "        bed_paths=bed_paths,\n",
    "        outdir=macs_outdir,\n",
    "        genome_size='hs',\n",
    "        n_cpu=N_CPU,\n",
    "        input_format='BEDPE',\n",
    "        keep_dup='all',\n",
    "        q_value=0.05,\n",
    "        _temp_dir='/tmp/scenic'\n",
    "    )\n",
    "print(\"OK: MACS2 peak calling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21db64a0",
   "metadata": {},
   "source": [
    "### **2.4. [耗时] 生成 Consensus Peak 集**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ee4a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检查点：检查最终的共识peak bed文件是否存在\n",
    "if os.path.exists(consensus_bed_path):\n",
    "    print(f\"INFO: Consensus peak file found at {consensus_bed_path}. Skipping.\")\n",
    "else:\n",
    "    print(\"INFO: Consensus peak file not found. Generating...\")\n",
    "    consensus_peaks = get_consensus_peaks(\n",
    "        narrow_peaks_dict=narrow_peak_dict,\n",
    "        peak_half_width=250,\n",
    "        chromsizes=chromsizes,\n",
    "        path_to_blacklist=blacklist_file\n",
    "    )\n",
    "    consensus_peaks.to_bed(path=consensus_bed_path, keep=True, compression='infer', chain=False)\n",
    "    print(f\"OK: Consensus peak file saved to {consensus_bed_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271ce733",
   "metadata": {},
   "source": [
    "### **2.5. [耗时] 运行 pycisTopic 质量控制 (QC)**\n",
    "\n",
    "获取TSS注释和运行核心QC流程。这里也给出了手动调整阈值的代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebc1d43-e559-4657-8e0b-518765c2d90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 核心QC流程\n",
    "\n",
    "# 定义QC流程的关键输入和输出文件路径\n",
    "qc_output_prefix = os.path.join(qc_dir, \"10x_multiome_brain\")\n",
    "\n",
    "# .fragments_stats_per_cb.parquet 文件是后续步骤的输入，是最佳选择。\n",
    "qc_main_output_file = f\"{qc_output_prefix}.fragments_stats_per_cb.parquet\"\n",
    "\n",
    "# --- 核心检查点：检查最终的QC结果是否存在 ---\n",
    "if os.path.exists(qc_main_output_file):\n",
    "    print(f\"INFO: Main QC output file found at {qc_main_output_file}. Skipping the entire QC step.\")\n",
    "    # 我们仍然需要确保 tss.bed 文件存在，以备后续其他步骤可能使用\n",
    "    if not os.path.exists(tss_bed_path):\n",
    "        print(f\"  - WARNING: Main QC output exists, but TSS annotation file is missing. Downloading it now...\")\n",
    "        cmd_tss = (\n",
    "            f\"pycistopic tss get_tss \"\n",
    "            f\"--output {tss_bed_path} \"\n",
    "            f\"--name hsapiens_gene_ensembl \"\n",
    "            f\"--to-chrom-source ucsc \"\n",
    "            f\"--ucsc hg38\"\n",
    "        )\n",
    "        os.system(cmd_tss)\n",
    "        print(\"  - OK: TSS annotation downloaded.\")\n",
    "else:\n",
    "    # --- 如果QC结果不存在，则执行所有必要的准备和计算 ---\n",
    "    print(f\"INFO: Main QC output file not found. Preparing inputs and running pycistopic qc...\")\n",
    "    \n",
    "    # 1. 准备TSS注释文件（现在是QC流程的一部分）\n",
    "    if os.path.exists(tss_bed_path):\n",
    "        print(f\"  - TSS annotation file found at {tss_bed_path}. Skipping download.\")\n",
    "    else:\n",
    "        print(f\"  - TSS annotation not found. Downloading for hg38...\")\n",
    "        cmd_tss = (\n",
    "            f\"pycistopic tss get_tss \"\n",
    "            f\"--output {tss_bed_path} \"\n",
    "            f\"--name hsapiens_gene_ensembl \"\n",
    "            f\"--to-chrom-source ucsc \"\n",
    "            f\"--ucsc hg38\"\n",
    "        )\n",
    "        os.system(cmd_tss)\n",
    "        print(\"  - OK: TSS annotation downloaded.\")\n",
    "\n",
    "    # 2. 运行核心QC流程\n",
    "    print(\"  - Running pycistopic qc...\")\n",
    "    cmd_qc = (\n",
    "        f\"pycistopic qc \"\n",
    "        f\"--fragments {fragments_file} \"\n",
    "        f\"--regions {consensus_bed_path} \"\n",
    "        f\"--tss {tss_bed_path} \"\n",
    "        f\"--output {qc_output_prefix}\"\n",
    "    )\n",
    "    os.system(cmd_qc)\n",
    "    \n",
    "    print(\"\\nOK: pycisTopic QC complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01bbb442-cfaa-4a78-a0b0-38603b1d862e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [debug]: 打印出 metrics_df 的所有列名\n",
    "# Available columns in metrics DataFrame:\n",
    "# ['barcode_rank', 'total_fragments_count', 'log10_total_fragments_count', \n",
    "# 'unique_fragments_count', 'log10_unique_fragments_count', \n",
    "# 'total_fragments_in_peaks_count', 'log10_total_fragments_in_peaks_count', \n",
    "# 'unique_fragments_in_peaks_count', 'log10_unique_fragments_in_peaks_count', \n",
    "# 'fraction_of_fragments_in_peaks', 'duplication_count', 'duplication_ratio', \n",
    "# 'nucleosome_signal', 'tss_enrichment', 'pdf_values_for_tss_enrichment', \n",
    "# 'pdf_values_for_fraction_of_fragments_in_peaks', \n",
    "# 'pdf_values_for_duplication_ratio']\n",
    "\n",
    "# metrics_path = os.path.join(pycistopic_qc_output_dir, f'{sample_id}.fragments_stats_per_cb.parquet')\n",
    "# if not os.path.exists(metrics_path):\n",
    "#    raise FileNotFoundError(f\"QC metrics file not found: {metrics_path}\")\n",
    "# metrics_df = pl.read_parquet(metrics_path).to_pandas().set_index(\"CB\")\n",
    "    \n",
    "# print(\"\\nAvailable columns in metrics DataFrame:\")\n",
    "# print(metrics_df.columns.tolist())\n",
    "# print(\"\\nFirst 5 rows of metrics DataFrame:\")\n",
    "# print(metrics_df.head())\n",
    "# print(\"-\" * 20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18dd27-be86-409a-9f8f-ce51a93531ce",
   "metadata": {},
   "source": [
    "**可视化QC结果与细胞筛选**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216c0b5c-5c74-4cfa-827e-c22f346939c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 步骤 1: 定义路径和初始化变量 ---\n",
    "pycistopic_qc_output_dir = os.path.join(OUT_DIR, \"qc\") \n",
    "os.makedirs(pycistopic_qc_output_dir, exist_ok=True)\n",
    "\n",
    "# 初始化用于存储结果的字典\n",
    "sample_id_to_barcodes_passing_filters = {}\n",
    "sample_id_to_thresholds = {}\n",
    "\n",
    "# --- 步骤 2: 定义筛选策略 ---\n",
    "# 设置一个开关来决定使用哪种方法。\n",
    "USE_MANUAL_THRESHOLDS = True\n",
    "\n",
    "# 定义手动筛选的阈值\n",
    "manual_thresholds = {\n",
    "    'unique_fragments_threshold': 1000,\n",
    "    'tss_enrichment_threshold': 5,\n",
    "    'frip_threshold': 0\n",
    "}\n",
    "\n",
    "print(f\"INFO: QC output is located in: {pycistopic_qc_output_dir}\")\n",
    "if USE_MANUAL_THRESHOLDS:\n",
    "    print(\"INFO: Using MANUAL thresholds for cell filtering.\")\n",
    "else:\n",
    "    print(\"INFO: Using AUTOMATIC thresholds for cell filtering.\")\n",
    "\n",
    "\n",
    "# --- 步骤 3: 循环处理每个样本 ---\n",
    "for sample_id in fragments_dict:\n",
    "    print(f\"\\n--- Processing sample: {sample_id} ---\")\n",
    "    \n",
    "    # 3.1 绘制样本的整体 QC 统计图\n",
    "    print(\"Step 3.1: Plotting overall sample QC stats...\")\n",
    "    fig_sample_stats = plot_sample_stats(\n",
    "        sample_id=sample_id,\n",
    "        pycistopic_qc_output_dir=pycistopic_qc_output_dir\n",
    "    )\n",
    "    plt.show(fig_sample_stats)\n",
    "    plt.close(fig_sample_stats)\n",
    "\n",
    "    # 3.2 根据开关【计算】筛选阈值\n",
    "    if USE_MANUAL_THRESHOLDS:\n",
    "        # 如果使用手动，我们直接用定义好的字典\n",
    "        thresholds = manual_thresholds\n",
    "        print(\"Step 3.2: Using pre-defined MANUAL thresholds...\")\n",
    "    else:\n",
    "        # 如果使用自动，我们调用函数来获取自动计算的阈值\n",
    "        print(\"Step 3.2: Calculating AUTOMATIC thresholds...\")\n",
    "        _, thresholds = get_barcodes_passing_qc_for_sample(\n",
    "            sample_id=sample_id,\n",
    "            pycistopic_qc_output_dir=pycistopic_qc_output_dir,\n",
    "            use_automatic_thresholds=True\n",
    "        )\n",
    "    \n",
    "    # 3.3 手动应用阈值进行筛选，以保留原始 barcode 格式\n",
    "    # 原版本用 get_barcodes_passing_qc_for_sample 获得barcode，但是好像会自动处理去掉-1标签，导致后续问题\n",
    "    # 现在尝试从原始文件中读取\n",
    "    print(\"Step 3.3: Applying thresholds to filter cells while preserving original barcode format...\")\n",
    "    \n",
    "    # 加载完整的 QC 指标数据\n",
    "    metrics_path = os.path.join(pycistopic_qc_output_dir, f'{sample_id}.fragments_stats_per_cb.parquet')\n",
    "    if not os.path.exists(metrics_path):\n",
    "        raise FileNotFoundError(f\"QC metrics file not found: {metrics_path}\")\n",
    "    metrics_df = pl.read_parquet(metrics_path).to_pandas().set_index(\"CB\")\n",
    "    \n",
    "    # 从字典中获取阈值\n",
    "    unique_fragments_thr = thresholds.get('unique_fragments_threshold', 0)\n",
    "    tss_enrichment_thr = thresholds.get('tss_enrichment_threshold', 0)\n",
    "    frip_thr = thresholds.get('frip_threshold', 0)\n",
    "    \n",
    "    # 进行筛选\n",
    "    passing_mask = (\n",
    "        (metrics_df['unique_fragments_count'] >= unique_fragments_thr) &\n",
    "        (metrics_df['tss_enrichment'] >= tss_enrichment_thr) &\n",
    "        (metrics_df['fraction_of_fragments_in_peaks'] >= frip_thr)\n",
    "    )\n",
    "    \n",
    "    # 获取通过筛选的、格式完整的 barcodes\n",
    "    barcodes = metrics_df[passing_mask].index.tolist()\n",
    "    \n",
    "    # 3.4 打印并存储结果\n",
    "    n_cells_passing = len(barcodes)\n",
    "    print(f\"INFO: Found {n_cells_passing} cells passing QC for sample '{sample_id}'.\")\n",
    "    print(f\"INFO: Applied thresholds: {thresholds}\")\n",
    "    \n",
    "    # 打印一些样本 barcode，以供验证\n",
    "    # print(\"Sample of filtered barcodes (format preserved):\", barcodes[:5])\n",
    "    \n",
    "    sample_id_to_barcodes_passing_filters[sample_id] = barcodes\n",
    "    sample_id_to_thresholds[sample_id] = thresholds\n",
    "\n",
    "    # 3.5 绘制筛选后的 barcode 统计图\n",
    "    print(\"Step 3.5: Plotting barcode-level stats with applied thresholds...\")\n",
    "    fig_barcode_stats = plot_barcode_stats(\n",
    "        sample_id=sample_id,\n",
    "        pycistopic_qc_output_dir=pycistopic_qc_output_dir,\n",
    "        bc_passing_filters=barcodes, # 使用我们自己筛选的列表\n",
    "        detailed_title=False,\n",
    "        **thresholds\n",
    "    )\n",
    "    plt.show(fig_barcode_stats)\n",
    "    plt.close(fig_barcode_stats)\n",
    "\n",
    "print(\"\\n--- QC and filtering complete for all samples. ---\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed76da0c",
   "metadata": {},
   "source": [
    "### **2.6. [耗时] 创建、筛选和保存 cisTopic 对象**\n",
    "\n",
    "这是将所有ATAC数据整合到一个核心对象中的关键步骤。\n",
    "\n",
    "这里需要额外关注一下barcode格式。在 10x Genomics 的单细胞测序技术中，一个细胞会被封装到一个凝胶珠（GEM, Gel Bead in Emulsion）中。-1 这个后缀代表的就是 GEM group 1。\n",
    "- 单样本实验: 在一个标准的单样本实验中，所有细胞都来自同一个 GEM group，所以它们的 barcode 都会带上 -1 后缀。\n",
    "- 多样本混合 (CellPlex/Hashing): 如果你将多个样本混合在一起进行测序，并使用了细胞哈希（cell hashing）等技术，那么不同样本的细胞可能会被分配到不同的 GEM group，你可能会看到 -1, -2, -3 等不同的后缀。\n",
    "- 所以在单样本分析中，它不重要。而在多样本混合分析中，则非常重要。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bcd2b06-7092-4ec7-bed2-74517a2525d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [debug] Barcode 格式与交集验证\n",
    "# \n",
    "# 这是在进行数据合并前至关重要的一步。我们将验证 GEX (scRNA-seq) 和 ATAC (scATAC-seq) 两个数据集中，\n",
    "# 高质量细胞的条形码 (barcodes) 是否存在交集，以及它们的格式是否一致。\n",
    "# **预期格式**: 我们期望两边的 barcodes 都遵循标准的 10x Genomics 格式，即 `SEQUENCE-1`。\n",
    "\n",
    "\n",
    "# --- 1. 检查 GEX (adata) 的 Barcodes ---\n",
    "# 假设 adata.obs.index 已经是我们期望的 'SEQUENCE-1' 格式\n",
    "gex_barcodes_set = set(adata.obs.index)\n",
    "\n",
    "print(f\"--- GEX (adata) Barcode Analysis ---\")\n",
    "print(f\"Total GEX barcodes: {len(gex_barcodes_set)}\")\n",
    "print(\"Sample of GEX barcodes (expected format: 'SEQUENCE-1'):\")\n",
    "print(list(gex_barcodes_set)[:3])\n",
    "print(\"-\" * 40)\n",
    "\n",
    "\n",
    "# --- 2. 检查通过 ATAC QC 的 Barcodes ---\n",
    "sample_id = \"10x_multiome_brain\"\n",
    "# 从上一步的 QC 结果中获取通过筛选的 barcodes\n",
    "# 我们期望这里存储的是未经修改的、带有 '-1' 后缀的原始 barcodes\n",
    "atac_barcodes_passing_qc = sample_id_to_barcodes_passing_filters.get(sample_id)\n",
    "\n",
    "if atac_barcodes_passing_qc is None or len(atac_barcodes_passing_qc) == 0:\n",
    "    print(\"WARNING: No barcodes passed ATAC QC from the previous step.\")\n",
    "    atac_barcodes_set = set() \n",
    "else:\n",
    "    atac_barcodes_set = set(atac_barcodes_passing_qc)\n",
    "    \n",
    "    print(f\"--- ATAC (pycisTopic QC) Barcode Analysis ---\")\n",
    "    print(f\"Total ATAC barcodes passing QC: {len(atac_barcodes_set)}\")\n",
    "    print(\"Sample of ATAC barcodes (expected format: 'SEQUENCE-1'):\")\n",
    "    print(list(atac_barcodes_set)[:3])\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "# --- 3. 检查交集 ---\n",
    "common_barcodes = gex_barcodes_set.intersection(atac_barcodes_set)\n",
    "\n",
    "print(\"--- INTERSECTION ANALYSIS ---\")\n",
    "print(f\"Number of common barcodes found: {len(common_barcodes)}\")\n",
    "\n",
    "if len(common_barcodes) > 0:\n",
    "    print(\"✅ OK: Found common barcodes! The datasets can now be merged.\")\n",
    "    print(\"Sample of common barcodes:\")\n",
    "    print(list(common_barcodes)[:3])\n",
    "else:\n",
    "    print(\"❌ ERROR: No common barcodes found. There might be a fundamental mismatch between the datasets.\")\n",
    "    print(\"Please re-check the QC filtering step and the original data sources.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf84d6b-bcdc-4788-b12b-b0b0e011de50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义这一步的输出文件路径\n",
    "cistopic_obj_path = os.path.join(OUT_DIR, \"cistopic_obj.pkl\")\n",
    "\n",
    "# 检查点：如果对象已存在，则直接加载\n",
    "if os.path.exists(cistopic_obj_path):\n",
    "    print(f\"INFO: Found cistopic object at {cistopic_obj_path}. Loading from file.\")\n",
    "    with open(cistopic_obj_path, 'rb') as f:\n",
    "        cistopic_obj = pickle.load(f)\n",
    "else:\n",
    "    print(\"INFO: cistopic object not found. Starting creation process from scratch.\")\n",
    "    \n",
    "    # --- 步骤 1: 获取上一步筛选好的、格式正确的 ATAC barcodes ---\n",
    "    sample_id = \"10x_multiome_brain\"\n",
    "    barcodes_passing_atac_qc = sample_id_to_barcodes_passing_filters.get(sample_id)\n",
    "    if barcodes_passing_atac_qc is None or len(barcodes_passing_atac_qc) == 0:\n",
    "        raise ValueError(f\"Filtered barcodes for sample '{sample_id}' not found. Please run the QC step (2.5) first.\")\n",
    "    \n",
    "    # --- 步骤 2: 与 GEX 数据求交集，得到最终分析的细胞列表 ---\n",
    "    gex_barcodes = set(adata.obs.index)\n",
    "    common_barcodes = list(gex_barcodes.intersection(set(barcodes_passing_atac_qc)))\n",
    "\n",
    "    print(f\"INFO: Found {len(common_barcodes)} common high-quality cells to be used for object creation.\")\n",
    "    if len(common_barcodes) == 0:\n",
    "        raise ValueError(\"FATAL: No common cells found between GEX and quality-filtered ATAC.\")\n",
    "\n",
    "    # --- 步骤 3: 创建只包含共同细胞的 cisTopic 对象 ---\n",
    "    print(f\"INFO: Creating CistopicObject using {len(common_barcodes)} final cells...\")\n",
    "    cistopic_obj = create_cistopic_object_from_fragments(\n",
    "        path_to_fragments=fragments_dict[sample_id],\n",
    "        path_to_regions=consensus_bed_path,\n",
    "        path_to_blacklist=blacklist_file,\n",
    "        valid_bc=common_barcodes,\n",
    "        n_cpu=N_CPU\n",
    "    )\n",
    "    print(\"INFO: Initial CistopicObject created.\")\n",
    "    \n",
    "    # --- 步骤 4: 强制校准 cistopic_obj 的 barcode 格式 ---\n",
    "    print(\"INFO: Harmonizing cell barcodes by removing '___cisTopic' suffix...\")\n",
    "    cistopic_obj.cell_names = [bc.replace('___cisTopic', '') for bc in cistopic_obj.cell_names]\n",
    "    cistopic_obj.cell_data.index = cistopic_obj.cell_names\n",
    "    \n",
    "    # --- 步骤 5: 对齐并添加细胞注释 ---\n",
    "    print(\"INFO: Aligning with GEX data and adding metadata...\")\n",
    "    common_barcodes_after_creation = list(set(adata.obs.index) & set(cistopic_obj.cell_names))\n",
    "    adata_subset = adata[common_barcodes_after_creation, :].copy()\n",
    "    cistopic_obj = cistopic_obj.subset(common_barcodes_after_creation, copy=True)\n",
    "    cistopic_obj.add_cell_data(adata_subset.obs)\n",
    "    \n",
    "    # --- 步骤 6: 保存这个对象 ---\n",
    "    print(f\"INFO: Saving cistopic object to {cistopic_obj_path}\")\n",
    "    with open(cistopic_obj_path, 'wb') as f:\n",
    "        pickle.dump(cistopic_obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    notify_chord()\n",
    "\n",
    "\n",
    "# --- 验证对象 ---\n",
    "print(cistopic_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088e9133-b623-47f6-992c-3a43dcc47b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scrublet 总是崩溃退出，代码多年未更新，作者也不太回复问题；放弃这一步\n",
    "\n",
    "# # 这是最终输出对象的路径\n",
    "# final_cistopic_obj_path = os.path.join(OUT_DIR, \"cistopic_obj.pkl\")\n",
    "\n",
    "# # 检查点：如果最终对象已存在，则直接加载\n",
    "# if os.path.exists(final_cistopic_obj_path):\n",
    "#     print(f\"INFO: Found final cistopic object at {final_cistopic_obj_path}. Loading from file.\")\n",
    "#     with open(final_cistopic_obj_path, 'rb') as f:\n",
    "#         cistopic_obj = pickle.load(f)\n",
    "# else:\n",
    "#     # 确保上一步的对象已加载到内存\n",
    "#     if 'cistopic_obj' not in locals():\n",
    "#          raise NameError(\"Variable 'cistopic_obj' from pre-scrublet step not found. Please run the previous cell first.\")\n",
    "    \n",
    "#     print(\"INFO: Starting Scrublet for doublet detection (this may be slow and memory-intensive)...\")\n",
    "    \n",
    "#     # --- 双胞检测 (Scrublet) ---\n",
    "#     scrub = scr.Scrublet(cistopic_obj.fragment_matrix.T, expected_doublet_rate=0.1)\n",
    "#     doublet_scores, predicted_doublets = scrub.scrub_doublets(verbose=False)\n",
    "#     scrub.call_doublets(threshold=0.22)\n",
    "    \n",
    "#     scrublet_df = pd.DataFrame(\n",
    "#         [scrub.doublet_scores_obs_, scrub.predicted_doublets_],\n",
    "#         columns=cistopic_obj.cell_names,\n",
    "#         index=['Doublet_scores_fragments', 'Predicted_doublets_fragments']\n",
    "#     ).T\n",
    "#     cistopic_obj.add_cell_data(scrublet_df)\n",
    "\n",
    "#     # --- 移除双胞 ---\n",
    "#     print(\"INFO: Removing predicted doublets...\")\n",
    "#     singlets = cistopic_obj.cell_data[cistopic_obj.cell_data.Predicted_doublets_fragments == False].index.tolist()\n",
    "#     cistopic_obj = cistopic_obj.subset(singlets, copy=True)\n",
    "    \n",
    "#     # --- 保存最终对象 ---\n",
    "#     print(f\"INFO: Saving final, filtered cistopic object to {final_cistopic_obj_path}\")\n",
    "#     with open(final_cistopic_obj_path, 'wb') as f:\n",
    "#         pickle.dump(cistopic_obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "# # --- 验证最终对象 ---\n",
    "# n_cells = cistopic_obj.fragment_matrix.shape[0]\n",
    "# n_regions = cistopic_obj.fragment_matrix.shape[1]\n",
    "# print(f\"\\nOK: Final cisTopic object is ready with {n_cells} cells and {n_regions} regions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92b0933",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **第三部分：Topic 建模与分析**\n",
    "\n",
    "本部分使用LDA模型识别调控主题（topics），并进行后续的聚类和可视化。这部分很耗时（3-4小时），内存要求也高（20G+）。\n",
    "\n",
    "### **3.1. [极耗时] 运行 LDA 主题模型 (Mallet)**\n",
    "\n",
    "关于Topic数的选择：\n",
    "\n",
    "`pycisTopic` 的核心是主题建模（Topic Modeling），其目的是将复杂的染色质开放状态矩阵分解为一组“主题”（Topics）。每个主题代表一个共有的调控模式（co-accessible regions），而每个细胞则可以由这些主题的权重组合来表示。主题数的选择是此步骤的关键，它直接影响下游分析的效果：\n",
    "- 主题数过少：可能导致不同的生物学模式被混淆在同一主题中，无法有效区分细胞亚群。\n",
    "- 主题数过多：可能引入噪音，或将一个连贯的生物学过程打散到多个不相关的主题中。\n",
    "\n",
    "我们在此处选择一个较宽的范围（如 2 到 40）进行初步探索。后续步骤中，`pycisTopic` 提供了评估不同主题数模型的指标（如 `log-likelihood` 和 `topic coherence`），帮助我们选择一个最优或次优的主题数，用于最终的细胞和区域分析。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf09961",
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = os.path.join(IN_DIR, \"Mallet-202108/bin/mallet\")\n",
    "models_path = os.path.join(OUT_DIR, \"models.pkl\")\n",
    "mallet_output_dir = os.path.join(OUT_DIR, \"mallet_tutorial\")\n",
    "os.makedirs(mallet_output_dir, exist_ok=True)\n",
    "\n",
    "# 检查点：检查模型列表是否已保存\n",
    "if os.path.exists(models_path):\n",
    "    print(f\"INFO: Found saved models at {models_path}. Loading from file.\")\n",
    "    with open(models_path, 'rb') as f:\n",
    "        models = pickle.load(f)\n",
    "else:\n",
    "    print(\"INFO: Saved models not found. Running LDA with Mallet (this is very time-consuming).\")\n",
    "    # 设置Mallet所需内存\n",
    "    os.environ['MALLET_MEMORY'] = '40G' \n",
    "    models = run_cgs_models_mallet(\n",
    "        cistopic_obj,\n",
    "        n_topics=[2, 5, 10, 15, 20, 25, 30, 35, 40, 45, 50],\n",
    "        n_cpu=N_CPU,\n",
    "        n_iter=500,\n",
    "        random_state=555,\n",
    "        alpha=50,\n",
    "        alpha_by_topic=True,\n",
    "        eta=0.1,\n",
    "        tmp_path=\"/tmp\",\n",
    "        save_path=mallet_output_dir,\n",
    "        mallet_path=mallet_path,\n",
    "    )\n",
    "    print(f\"INFO: Saving models to {models_path}\")\n",
    "    with open(models_path, 'wb') as f:\n",
    "        pickle.dump(models, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    notify_chord()\n",
    "\n",
    "print(\"OK: LDA modeling complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077db5fa-39d2-4a05-bfd0-f437d172ee98",
   "metadata": {},
   "source": [
    "### **3.2. 模型评估与下游分析**\n",
    "\n",
    "使用 evaluate_models 函数计算并可视化模型的评估指标，这有助于我们选择一个最佳的主题数（n_topics）。plot_metrics=True 会自动生成四张图：\n",
    "1. Arun (2010) - 寻找“肘部”或最小值\n",
    "2. Cao et al. (2009) - 寻找最小值\n",
    "3. Log-likelihood - 寻找“肘部”或平台期\n",
    "4. Perplexity - 寻找“肘部”或平台期"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d42375",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估并选择最佳模型\n",
    "# `evaluate_models` 会生成一个图，这里我们直接选择一个模型\n",
    "print(\"INFO: Evaluating LDA models and selecting the best one...\")\n",
    "model = evaluate_models(models, select_model=40, return_model=True, plot_metrics=True)\n",
    "cistopic_obj.add_LDA_model(model)\n",
    "print(\"\\nModel Evaluation Metrics:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3a39bd-d228-41dc-ad6f-37cfc65a4fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 聚类与降维\n",
    "print(\"INFO: Finding clusters and running UMAP...\")\n",
    "run_umap(cistopic_obj, target='cell', scale=True)\n",
    "find_clusters(cistopic_obj, target='cell', k=15, res=[0.6, 1.2], prefix='pycisTopic_')\n",
    "\n",
    "\n",
    "plot_metadata(\n",
    "    cistopic_obj,\n",
    "    reduction_name='UMAP',\n",
    "    variables=['Seurat_cell_type', 'pycisTopic_leiden_15_0.6', 'pycisTopic_leiden_15_1.2'],\n",
    "    target='cell', \n",
    "    num_columns=3,\n",
    "    text_size=10,\n",
    "    dot_size=5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9d1c47-d225-4df7-b661-ab5530e6f2fd",
   "metadata": {},
   "source": [
    "### **3.3. [耗时] 可及性插补与差异分析 (DARs)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00d08ec-3086-49dd-8ba5-da98871afabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputed_acc_obj_path = os.path.join(OUT_DIR, 'imputed_accessibility.pkl')\n",
    "markers_dict_path = os.path.join(OUT_DIR, 'DARs_markers.pkl')\n",
    "\n",
    "# --- 步骤 1: 可及性插补 ---\n",
    "# 检查点：检查插补后的可及性矩阵\n",
    "if os.path.exists(imputed_acc_obj_path):\n",
    "    print(f\"INFO: Found imputed accessibility object at {imputed_acc_obj_path}. Loading.\")\n",
    "    with open(imputed_acc_obj_path, 'rb') as f:\n",
    "        imputed_acc_obj = pickle.load(f)\n",
    "else:\n",
    "    print(\"INFO: Imputed accessibility object not found. Computing...\")\n",
    "    imputed_acc_obj = impute_accessibility(cistopic_obj, scale_factor=10**6)\n",
    "    with open(imputed_acc_obj_path, 'wb') as f:\n",
    "        pickle.dump(imputed_acc_obj, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(\"OK: Imputation complete.\")\n",
    "    notify_chord()\n",
    "\n",
    "# --- 步骤 2: 差异可及区域 (DARs) 分析 ---\n",
    "# 检查点：检查差异可及区域的结果\n",
    "if os.path.exists(markers_dict_path):\n",
    "    print(f\"\\nINFO: Found DARs markers dict at {markers_dict_path}. Loading.\")\n",
    "    with open(markers_dict_path, 'rb') as f:\n",
    "        markers_dict = pickle.load(f)\n",
    "    # 加载后也检查一下是否为空\n",
    "    if not markers_dict or all(df.empty for df in markers_dict.values()):\n",
    "         print(\"WARNING: Loaded DARs markers dictionary is empty. This may indicate no significant results were found previously.\")\n",
    "else:\n",
    "    print(\"\\nINFO: DARs markers dict not found. Computing (this can be slow)...\")\n",
    "    markers_dict = find_diff_features(\n",
    "        cistopic_obj,\n",
    "        imputed_acc_obj,\n",
    "        variable='Seurat_cell_type',\n",
    "        adjpval_thr=0.05,\n",
    "        log2fc_thr=np.log2(1.5),\n",
    "        n_cpu=N_CPU,\n",
    "        split_pattern='-'\n",
    "    )\n",
    "    \n",
    "    # 在保存前检查结果是否为空\n",
    "    if not markers_dict or all(df.empty for df in markers_dict.values()):\n",
    "        # 如果 markers_dict 是一个空字典，或者字典里所有的 DataFrame 都是空的\n",
    "        print(\"\\nWARNING: find_diff_features did not return any significant DARs with the current thresholds.\")\n",
    "        print(\"An empty markers dictionary will be saved, but downstream analysis may fail.\")\n",
    "    else:\n",
    "        print(\"\\nINFO: Significant DARs found. Saving results...\")\n",
    "\n",
    "    # 保存结果（即使是空的），以避免下次重复计算\n",
    "    with open(markers_dict_path, 'wb') as f:\n",
    "        pickle.dump(markers_dict, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    notify_chord()\n",
    "\n",
    "print(\"\\nOK: Imputation and DAR analysis complete.\")\n",
    "\n",
    "# --- 步骤 3: 打印结果摘要 ---\n",
    "print(\"\\n--- Summary of Differential Accessibility Analysis ---\")\n",
    "if 'markers_dict' in locals() and markers_dict:\n",
    "    for cell_type, dar_df in markers_dict.items():\n",
    "        print(f\"  - Cell type '{cell_type}': Found {len(dar_df)} significant DARs.\")\n",
    "else:\n",
    "    print(\"  - No significant DARs were found across all cell types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e975bde-22bb-4d59-ac18-a89963fa32b2",
   "metadata": {},
   "source": [
    "### **3.4. 差异可及性区域 (DARs) 可视化**\n",
    "\n",
    "在计算出每个细胞类型的差异可及区域（DARs）后，我们可以通过多种方式对其进行可视化，以直观地理解结果。\n",
    "\n",
    "**1. 火山图 (Volcano Plot)**\n",
    "\n",
    "火山图可以直观地展示在特定细胞类型中，哪些区域是显著上调（或下调）的。x轴是 log2 Fold Change，y轴是 -log10(p-value_adj)。\n",
    "\n",
    "你可以从 markers_dict.keys() 中查看所有可用的细胞类型。Available types: \n",
    "`'AST', 'BG', 'COP', 'ENDO', 'GC', 'GP', 'INH_SNCG', 'INH_SST', 'INH_VIP', 'MG', 'MGL', 'MOL', 'NFOL', 'OPC', 'PURK']`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc9421e-a4fb-4e05-88ad-147993a3bfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 定义要可视化的细胞类型列表 ---\n",
    "# 你可以在这里添加或删除任何你感兴趣的细胞类型\n",
    "cell_types_to_plot = ['OPC', 'MOL'] \n",
    "\n",
    "# --- 2. 检查 markers_dict 是否存在且非空 ---\n",
    "if 'markers_dict' in locals() and markers_dict:\n",
    "    \n",
    "    # --- 3. 循环遍历列表，为每个细胞类型绘图 ---\n",
    "    for cell_type in cell_types_to_plot:\n",
    "        \n",
    "        # 检查当前细胞类型的结果是否存在\n",
    "        if cell_type not in markers_dict or markers_dict[cell_type].empty:\n",
    "            print(f\"\\nWARNING: No differential regions found for cell type '{cell_type}'. Skipping plot.\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n--- Generating Volcano Plot for: {cell_type} ---\")\n",
    "        \n",
    "        # 获取该细胞类型的差异分析结果\n",
    "        dar_df = markers_dict[cell_type].copy() # 使用 .copy() 避免后续操作修改原始字典\n",
    "        \n",
    "        # 确保我们使用正确的列名\n",
    "        # 假设 p-value 列是 'Adjusted_pval', log2FC 列是 'Log2FC'\n",
    "        pval_col = 'Adjusted_pval'\n",
    "        logfc_col = 'Log2FC'\n",
    "        \n",
    "        if pval_col not in dar_df.columns or logfc_col not in dar_df.columns:\n",
    "            print(f\"ERROR: DataFrame for '{cell_type}' is missing required columns. Available: {list(dar_df.columns)}\")\n",
    "            continue\n",
    "\n",
    "        # 创建一个新的列来标记显著的 DARs\n",
    "        logfc_threshold = 0.5\n",
    "        pval_threshold = 0.05\n",
    "        dar_df['significant'] = (dar_df[pval_col] < pval_threshold) & (abs(dar_df[logfc_col]) > logfc_threshold)\n",
    "        \n",
    "        # 为了绘图，将 p值为0 的情况替换为一个非常小的数\n",
    "        dar_df['-log10_pval'] = -np.log10(dar_df[pval_col].replace(0, np.finfo(float).tiny))\n",
    "\n",
    "        # --- 开始绘图 ---\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        \n",
    "        plot = sns.scatterplot(\n",
    "            data=dar_df,\n",
    "            x=logfc_col,\n",
    "            y='-log10_pval',\n",
    "            hue='significant',\n",
    "            palette={True: 'red', False: 'grey'},\n",
    "            edgecolor=None,\n",
    "            s=15,\n",
    "            alpha=0.6,\n",
    "            legend=False\n",
    "        )\n",
    "        \n",
    "        # 添加阈值线\n",
    "        plt.axhline(-np.log10(pval_threshold), color='blue', linestyle='--', linewidth=1)\n",
    "        plt.axvline(logfc_threshold, color='blue', linestyle='--', linewidth=1)\n",
    "        plt.axvline(-logfc_threshold, color='blue', linestyle='--', linewidth=1)\n",
    "        \n",
    "        # 添加标题和标签\n",
    "        plt.title(f'Volcano Plot for DARs in {cell_type}', fontsize=16)\n",
    "        plt.xlabel('Log2 Fold Change', fontsize=12)\n",
    "        plt.ylabel('-log10 (Adjusted P-value)', fontsize=12)\n",
    "        \n",
    "        # 计算并标注显著上调和下调的区域数量\n",
    "        n_up = dar_df[(dar_df['significant']) & (dar_df[logfc_col] > 0)].shape[0]\n",
    "        n_down = dar_df[(dar_df['significant']) & (dar_df[logfc_col] < 0)].shape[0]\n",
    "        \n",
    "        plt.text(0.95, 0.95, f'Up: {n_up}', ha='right', va='top', transform=plot.transAxes, color='red', fontsize=12)\n",
    "        plt.text(0.05, 0.95, f'Down: {n_down}', ha='left', va='top', transform=plot.transAxes, color='red', fontsize=12)\n",
    "        \n",
    "        plt.grid(False)\n",
    "        sns.despine()\n",
    "        \n",
    "        # 显示图像\n",
    "        plt.show()\n",
    "\n",
    "else:\n",
    "    print(\"WARNING: 'markers_dict' not found or is empty. Cannot generate volcano plots.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a3b357-3f2d-4b20-9bd9-983237024b76",
   "metadata": {},
   "source": [
    "**2. UMAP 特征图 (Feature Plot)**\n",
    "\n",
    "我们可以选择每个细胞类型中最重要的一个或几个 DARs (通常是 Log2FC 最高且 adj_pval 最低的)，然后在 UMAP 空间上可视化这些区域的可及性，看看它们是否确实在该细胞类型中特异性地开放。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4f9a0b-89c4-4e79-b491-0e428663a07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nINFO: Plotting top DARs on UMAP...\")\n",
    "\n",
    "# 从每个我们感兴趣的细胞类型中，选择 Log2FC 最高的那个 DAR\n",
    "features_to_plot = []\n",
    "cell_types_of_interest = ['MOL', 'OPC'] # 选择几个代表性的细胞类型\n",
    "\n",
    "for cell_type in cell_types_of_interest:\n",
    "    if cell_type in markers_dict and not markers_dict[cell_type].empty:\n",
    "        # 按 Log2FC 降序排序，并取第一个（即最上调的）DAR 的名字\n",
    "        top_dar = markers_dict[cell_type].sort_values('Log2FC', ascending=False).index[0]\n",
    "        features_to_plot.append(top_dar)\n",
    "\n",
    "if features_to_plot:\n",
    "    # 使用 pycisTopic 自带的绘图函数来可视化这些区域的“插补后可及性”\n",
    "    plot_imputed_features(\n",
    "        cistopic_obj,\n",
    "        reduction_name='UMAP',\n",
    "        imputed_data=imputed_acc_obj, # 使用之前计算的插补矩阵\n",
    "        features=features_to_plot,\n",
    "        scale=True, # 对数值进行缩放以便更好地可视化\n",
    "        num_columns=4\n",
    "    )\n",
    "else:\n",
    "    print(\"WARNING: Could not find top DARs for the specified cell types. Skipping UMAP plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc6385d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **第四部分：SCENIC+ Snakemake 流程**\n",
    "\n",
    "SCENIC+的核心分析是通过一个强大的工作流管理工具Snakemake来执行的。在Notebook中的主要任务是**准备输入文件**和**生成配置文件**，然后提供在终端中运行Snakemake的命令。\n",
    "\n",
    "这部分很耗时（4-6小时），内存要求也极高（60G+）。个人电脑运行不现实，只能加载示范数据用于可视化，实际使用需要高配置服务器。\n",
    "\n",
    "### **4.1. 准备SCENIC+输入**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8a74f2-a46c-41ff-817a-7ed33ac5daae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 初始化SCENIC+工作目录\n",
    "# 注意：因为环境的复杂性，这个命令最好手动在容器中运行，而不是直接在notebook中运行。\n",
    "\n",
    "print(\"--- ACTION REQUIRED: Please run the following commands in your terminal ---\")\n",
    "print(\"\\n# Set environment variable to suppress warnings and run initialization:\")\n",
    "print(f'export PYTHONWARNINGS=\"ignore:pkg_resources is deprecated as an API\" && scenicplus init_snakemake --out_dir {SCPLUS_DIR}')\n",
    "print(\"\\n--------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b0302-622d-4b0a-9035-20ae7a6ae63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 准备 Region Sets**\n",
    "\n",
    "# --- 1. 创建目录结构 ---\n",
    "region_sets_base_dir = os.path.join(OUT_DIR, \"region_sets\")\n",
    "topics_otsu_dir = os.path.join(region_sets_base_dir, \"Topics_otsu\")\n",
    "dars_dir = os.path.join(region_sets_base_dir, \"DARs_cell_type\")\n",
    "\n",
    "os.makedirs(topics_otsu_dir, exist_ok=True)\n",
    "os.makedirs(dars_dir, exist_ok=True)\n",
    "\n",
    "# --- 2. 从 Topics 生成 Region sets ---\n",
    "print(\"INFO: Generating region sets from binarized topics (Otsu method)...\")\n",
    "region_bin_topics_otsu = binarize_topics(cistopic_obj, method='otsu')\n",
    "\n",
    "for topic in region_bin_topics_otsu:\n",
    "    # 检查是否有区域\n",
    "    if region_bin_topics_otsu[topic].index.empty:\n",
    "        print(f\"  - WARNING: Topic {topic} has no regions after binarization. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # 转换、排序、保存\n",
    "    region_names_to_coordinates(\n",
    "        region_bin_topics_otsu[topic].index\n",
    "    ).sort_values(\n",
    "        [\"Chromosome\", \"Start\", \"End\"]\n",
    "    ).to_csv(\n",
    "        os.path.join(topics_otsu_dir, f\"{topic}.bed\"),\n",
    "        sep=\"\\t\",\n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "# --- 3. 从 DARs 生成 Region sets ---\n",
    "print(\"\\nINFO: Generating region sets from DARs...\")\n",
    "for cell_type in markers_dict:\n",
    "    # 检查是否有区域\n",
    "    if markers_dict[cell_type].index.empty:\n",
    "        print(f\"  - WARNING: No DARs found for '{cell_type}', skipping .bed file generation.\")\n",
    "        continue\n",
    "    \n",
    "    # 转换、排序、保存\n",
    "    region_names_to_coordinates(\n",
    "        markers_dict[cell_type].index\n",
    "    ).sort_values(\n",
    "        [\"Chromosome\", \"Start\", \"End\"]\n",
    "    ).to_csv(\n",
    "        os.path.join(dars_dir, f\"{cell_type}.bed\"),\n",
    "        sep=\"\\t\",\n",
    "        header=False, \n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(f\"\\nOK: Region sets for SCENIC+ saved in subdirectories under {region_sets_base_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d09c6d07-868d-4875-980b-b43f752f2325",
   "metadata": {},
   "source": [
    "### **4.2. 配置 `config.yaml` 文件**\n",
    "\n",
    "SCENIC+ 的 Snakemake 工作流由 `config/config.yaml` 文件驱动。下面的代码将自动读取由 `scenicplus init_snakemake` 生成的模板，并填入正确的输入文件路径。\n",
    "\n",
    "**重要前提：准备输入文件**\n",
    "\n",
    "在运行下面的代码之前，请确保所有必需的输入文件都已准备就绪：\n",
    "\n",
    "*   **在 `input/` 目录下 (预先提供或下载的文件):**\n",
    "    *   从 [cistarget](https://resources.aertslab.org/cistarget/databases/) 数据库文件（两个feather文件），见 Notebook 第一节说明。\n",
    "\n",
    "*   **在 `output/` 目录下 (由本 Notebook 前序步骤生成的文件):**\n",
    "    *   `cistopic_obj.pkl`\n",
    "    *   `scRNA.h5ad`\n",
    "    *   `region_sets` 目录\n",
    "\n",
    "所有由 Snakemake 生成的**新文件**，将会被输出到 `scplus_pipeline/` 目录中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd32c6a-71a3-45af-9af3-244675018f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [debug] 不读取整个文件的前提下检查 feather 文件。平常不用运行。\n",
    "# ipc.open_file 可获得块数和列数；获取行数需要读一列后再获取\n",
    "\n",
    "CHECK_FEATHER = False\n",
    "\n",
    "if CHECK_FEATHER:\n",
    "\n",
    "    import pyarrow\n",
    "    import pyarrow.feather\n",
    "    \n",
    "    # infile = \"mm10_screen_v10_clust.regions_vs_motifs.scores.feather\"\n",
    "    infile = \"hg38_screen_v10_clust.regions_vs_motifs.scores.feather\"\n",
    "    \n",
    "    file_path = os.path.join(IN_DIR, infile)\n",
    "    reader = pyarrow.ipc.open_file(file_path)\n",
    "    \n",
    "    num_chunks = reader.num_record_batches\n",
    "    print(f\"块数: {num_chunks}\")\n",
    "    \n",
    "    colnames = reader.schema.names\n",
    "    num_cols = len(colnames)\n",
    "    print(f\"列数: {num_cols:,}\")\n",
    "    \n",
    "    print(f\"列名：{colnames[:3]} ...\")\n",
    "    \n",
    "    # 查找非 chrXXX 的列 => 有一列 motifs\n",
    "    # non_chr_columns = []\n",
    "    # for col in colnames:\n",
    "    #     if not col.startswith('chr'):\n",
    "    #         non_chr_columns.append(col)\n",
    "    # if non_chr_columns:\n",
    "    #     print(f\"Found {len(non_chr_columns)} column(s) that DO NOT start with 'chr'.\")\n",
    "    #     print(\"Sample of non-chr columns:\", non_chr_columns[:min(10, len(non_chr_columns))])\n",
    "    # else:\n",
    "    #     print(\"✅ OK: All column names start with 'chr'.\")\n",
    "    \n",
    "    table_slice = pyarrow.feather.read_table(file_path, columns=[colnames[0]])\n",
    "    # table_slice = pyarrow.feather.read_table(file_path, columns=['motifs'])\n",
    "    num_rows = table_slice.num_rows\n",
    "    print(f\"行数: {num_rows:,}\")\n",
    "    # print(table_slice[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1397f4d-07f0-4153-be54-11f68edae746",
   "metadata": {},
   "source": [
    "**检查所有文件，并生成 Snakemake 的配置文件**\n",
    "1.  验证所有必需的输入文件是否都已正确生成。\n",
    "2.  检查关键文件（如 `cistopic_obj.pkl`）的内容是否完整。\n",
    "3.  如果所有检查通过，则自动生成一个路径正确、参数完整的 `config.yaml` 文件。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db75106-827d-426f-a5e4-c9d6cc36186b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. 定义所有路径 ---\n",
    "config_yaml_path = os.path.join(SCPLUS_DIR, 'Snakemake', 'config', 'config.yaml')\n",
    "cistopic_obj_fname = os.path.abspath(os.path.join(OUT_DIR, \"cistopic_obj.pkl\"))\n",
    "gex_anndata_fname = os.path.abspath(os.path.join(OUT_DIR, \"scRNA.h5ad\"))\n",
    "region_sets_base_dir = os.path.abspath(os.path.join(OUT_DIR, \"region_sets\"))\n",
    "ctx_db_fname = os.path.abspath(os.path.join(IN_DIR, \"hg38_screen_v10_clust.regions_vs_motifs.rankings.feather\"))\n",
    "dem_db_fname = os.path.abspath(os.path.join(IN_DIR, \"hg38_screen_v10_clust.regions_vs_motifs.scores.feather\"))\n",
    "motif_annotations_fname = os.path.abspath(os.path.join(IN_DIR, \"motifs-v10nr_clust-nr.hgnc-m0.001-o0.0.tbl\"))\n",
    "\n",
    "# --- 2. 执行输入文件的完整性检查 ---\n",
    "print(\"--- Running Pre-Snakemake Sanity Checks ---\")\n",
    "all_checks_passed = True\n",
    "\n",
    "# 2.1 检查基础文件和目录是否存在\n",
    "print(\"[Check 2.1] Verifying file and directory existence...\", end=' ')\n",
    "errors = []\n",
    "input_files_to_check = {\n",
    "    \"cisTopic Object\": cistopic_obj_fname, \"GEX AnnData\": gex_anndata_fname,\n",
    "    \"Region Sets Base Folder\": region_sets_base_dir, \"cisTarget Rankings DB\": ctx_db_fname,\n",
    "    \"cisTarget Scores DB\": dem_db_fname, \"Motif Annotations\": motif_annotations_fname,\n",
    "}\n",
    "for name, path in input_files_to_check.items():\n",
    "    if not os.path.exists(path):\n",
    "        errors.append(f\"'{name}' not found\")\n",
    "if errors:\n",
    "    print(f\"❌ ERROR: {', '.join(errors)}\")\n",
    "    all_checks_passed = False\n",
    "else:\n",
    "    print(\"✅\")\n",
    "\n",
    "# 2.2 检查 region_sets 子目录的内容\n",
    "print(\"[Check 2.2] Validating 'region_sets' content and structure...\", end=' ')\n",
    "errors = []\n",
    "if os.path.exists(region_sets_base_dir):\n",
    "    topics_dir = os.path.join(region_sets_base_dir, \"Topics_otsu\")\n",
    "    dars_dir = os.path.join(region_sets_base_dir, \"DARs_cell_type\")\n",
    "    \n",
    "    if not os.path.isdir(topics_dir): errors.append(\"'Topics_otsu' subdir missing\")\n",
    "    else:\n",
    "        files = glob.glob(os.path.join(topics_dir, \"*.bed\"));\n",
    "        if not files: errors.append(\"No .bed files in 'Topics_otsu'\")\n",
    "        elif os.path.getsize(files[0]) == 0: errors.append(\"First .bed in 'Topics_otsu' is empty\")\n",
    "\n",
    "    if not os.path.isdir(dars_dir): errors.append(\"'DARs_cell_type' subdir missing\")\n",
    "    else:\n",
    "        files = glob.glob(os.path.join(dars_dir, \"*.bed\"));\n",
    "        if not files and 'markers_dict' in locals() and any(not df.empty for df in markers_dict.values()):\n",
    "            errors.append(\"No .bed files in 'DARs_cell_type' (but were expected)\")\n",
    "        elif files and os.path.getsize(files[0]) == 0: errors.append(\"First .bed in 'DARs_cell_type' is empty\")\n",
    "else:\n",
    "    errors.append(\"'region_sets' base directory not found\")\n",
    "if errors:\n",
    "    print(f\"❌ ERROR: {', '.join(errors)}\")\n",
    "    all_checks_passed = False\n",
    "else:\n",
    "    print(\"✅\")\n",
    "\n",
    "# 2.3 检查 cisTopic 对象的内容（模型和染色体命名）\n",
    "print(\"[Check 2.3] Validating 'cistopic_obj.pkl' content...\", end=' ')\n",
    "errors = []\n",
    "if os.path.exists(cistopic_obj_fname):\n",
    "    with open(cistopic_obj_fname, 'rb') as f: temp_cistopic_obj = pickle.load(f)\n",
    "    if temp_cistopic_obj.selected_model is None:\n",
    "        errors.append(\"No selected LDA model found\")\n",
    "    sample_regions = temp_cistopic_obj.region_names[:10]\n",
    "    if sample_regions and not all(r.startswith('chr') for r in sample_regions):\n",
    "        errors.append(f\"Region names do not use 'chr' prefix (e.g., '{sample_regions[0]}')\")\n",
    "    del temp_cistopic_obj\n",
    "else:\n",
    "    errors.append(\"'cistopic_obj.pkl' not found\")\n",
    "if errors:\n",
    "    print(f\"❌ ERROR: {', '.join(errors)}\")\n",
    "    all_checks_passed = False\n",
    "else:\n",
    "    print(\"✅\")\n",
    "\n",
    "# 2.4 检查 Barcode 一致性\n",
    "print(\"[Check 2.4] Verifying barcode consistency...\", end=' ')\n",
    "if 'adata' in locals() and 'cistopic_obj' in locals():\n",
    "    gex_barcodes = set(adata.obs.index); atac_barcodes = set(cistopic_obj.cell_data.index)\n",
    "    common_cells = len(gex_barcodes.intersection(atac_barcodes))\n",
    "    if common_cells == 0:\n",
    "        print(f\"❌ ERROR: No common cells between GEX ({len(gex_barcodes)}) and ATAC ({len(atac_barcodes)}).\")\n",
    "        all_checks_passed = False\n",
    "    else:\n",
    "        print(f\"Found {common_cells} common cells. ✅\")\n",
    "else:\n",
    "    print(\"WARNING: 'adata' or 'cistopic_obj' not in memory, skipping.\")\n",
    "\n",
    "# --- 3. 如果检查失败，则停止；否则，生成配置文件 ---\n",
    "if not all_checks_passed:\n",
    "    raise ValueError(\"\\n\\nOne or more critical pre-flight checks failed. Please review the errors above and re-run the necessary notebook sections.\")\n",
    "else:\n",
    "    print(\"\\n✅ OK: All pre-flight checks passed! Proceeding to generate config.yaml.\")\n",
    "    with open(config_yaml_path, 'r') as f: config_dict = yaml.safe_load(f)\n",
    "    input_data_to_update = {\n",
    "        'cisTopic_obj_fname': cistopic_obj_fname, 'GEX_anndata_fname': gex_anndata_fname,\n",
    "        'region_set_folder': region_sets_base_dir, 'ctx_db_fname': ctx_db_fname,\n",
    "        'dem_db_fname': dem_db_fname, 'path_to_motif_annotations': motif_annotations_fname,\n",
    "    }\n",
    "    params_general_to_update = {'n_cpu': 16, 'temp_dir': \"/tmp/scenicplus\"}\n",
    "    params_data_prep_to_update = {'species': \"hsapiens\", 'key_to_group_by': \"Seurat_cell_type\"}\n",
    "    params_motif_enrichment_to_update = {'species': \"homo_sapiens\"}\n",
    "    config_dict['input_data'].update(input_data_to_update)\n",
    "    config_dict['params_general'].update(params_general_to_update)\n",
    "    config_dict['params_data_preparation'].update(params_data_prep_to_update)\n",
    "    config_dict['params_motif_enrichment'].update(params_motif_enrichment_to_update)\n",
    "    with open(config_yaml_path, 'w') as f: yaml.dump(config_dict, f, default_flow_style=False, sort_keys=False)\n",
    "    print(\"OK: `config.yaml` has been successfully updated and validated.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750a9bfb-6420-42f9-97e6-eda84ab4b706",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### **4.3. [极耗时] 运行 SCENIC+**\n",
    "\n",
    "配置好`config.yaml`后，在终端（Terminal）中进入`scplus_pipeline`目录并运行Snakemake。\n",
    "\n",
    "不建议直接在Notebook中运行，因为日志输出和错误处理在终端中更清晰。\n",
    "\n",
    "注意：这一步可能非常耗时（3-5小时）、耗内存（60G+），测试时最好用单线程并注意监视内存使用情况。\n",
    "\n",
    "```bash\n",
    "# 在你的终端中执行以下命令:\n",
    "\n",
    "# 1. 激活 scenicplus 环境\n",
    "conda activate scenicplus\n",
    "\n",
    "# 2. 切换到工作目录\n",
    "cd ~/project/scplus_pipeline/Snakemake\n",
    "\n",
    "# 3. 运行Snakemake (推荐使用 --use-conda 来管理依赖)\n",
    "# 例如，使用5个核心:\n",
    "snakemake -c 5 --use-conda\n",
    "\n",
    "# 如果有运行错误的话，应该放弃任务并行，这样容易排查问题\n",
    "snakemake -c 1 --use-conda\n",
    "\n",
    "# 另外，还可以提前抑制一些报警信息\n",
    "export PYTHONWARNINGS=\"ignore:pkg_resources is deprecated as an API\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68430ecd",
   "metadata": {},
   "source": [
    "\n",
    "## **第五部分：结果探索**\n",
    "\n",
    "在成功运行 SCENIC+ Snakemake 流程后，所有的核心结果都储存在 `scplus_pipeline/Snakemake/scplusmdata.h5mu` 文件中。本节将展示如何加载此文件，并进行下游的探索性分析和可视化。\n",
    "\n",
    "### **5.1. 环境设置与加载结果**\n",
    "\n",
    "首先，我们需要加载必要的库和 `scplusmdata.h5mu` 文件。这个 MuData 对象是后续所有分析的起点。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7327deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mudata\n",
    "from scenicplus.RSS import regulon_specificity_scores, plot_rss\n",
    "from scenicplus.plotting.dotplot import heatmap_dotplot\n",
    "\n",
    "mdata_path = os.path.join(SCPLUS_DIR, 'Snakemake', 'scplusmdata.h5mu')\n",
    "\n",
    "# 检查文件是否存在\n",
    "if not os.path.exists(mdata_path):\n",
    "    raise FileNotFoundError(f\"MuData result file not found at: {mdata_path}\")\n",
    "\n",
    "# 加载 MuData 对象\n",
    "print(\"SCENIC+ MuData object loading:\")\n",
    "scplus_mdata = mudata.read(mdata_path)\n",
    "print(scplus_mdata)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303e400a-9510-4243-b4a7-af9a841b74aa",
   "metadata": {},
   "source": [
    "### **5.2. 探索 eRegulon 元数据**\n",
    "\n",
    "eRegulon（增强的调控子）是 SCENIC+ 的核心概念，它包含了从转录因子（TF）到其调控的区域（Region）再到目标基因（Gene）的完整链接。这些信息储存在 `.uns` 属性中。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7848690-6d19-4979-82bb-1c9d613a5e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看直接 eRegulon 的元数据\n",
    "# \"direct\" 指的是 TF 的 motif 直接出现在目标区域的增强子上\n",
    "e_regulons_meta = scplus_mdata.uns[\"direct_e_regulon_metadata\"]\n",
    "\n",
    "print(\"Direct eRegulons metadata (top 5 rows):\")\n",
    "display(e_regulons_meta.head())\n",
    "\n",
    "# 我们可以基于此数据框进行查询\n",
    "# 例如，查找转录因子 BCL11A 调控的最重要的几个目标基因（按 triplet_rank 排序）\n",
    "print(\"\\nTop 5 targets for TF 'BCL11A' based on triplet_rank:\")\n",
    "display(\n",
    "    e_regulons_meta[e_regulons_meta['TF'] == 'BCL11A']\n",
    "    .sort_values('triplet_rank')\n",
    "    .head()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9471d0e-8881-41ec-8a35-ab60e9b4a2c3",
   "metadata": {},
   "source": [
    "### **5.3. 基于 eRegulon 活性的 UMAP 可视化**\n",
    "\n",
    "我们可以使用 eRegulon 的 AUC（Area Under the Curve）富集分数来计算细胞的降维表征。这可以揭示细胞是否根据其活跃的调控网络进行聚类。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44139136-f8d4-4786-b686-384ee67f0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 合并 direct 和 extended eRegulons 的基因富集分数\n",
    "eRegulon_auc = anndata.concat(\n",
    "    [scplus_mdata[\"direct_gene_based_AUC\"], scplus_mdata[\"extended_gene_based_AUC\"]],\n",
    "    axis=1,\n",
    "    join='outer', # 使用 outer join 避免因 regulon 名称不完全匹配而出错\n",
    "    fill_value=0  # 用0填充缺失值\n",
    ")\n",
    "\n",
    "# 将主对象的细胞注释信息传递给新的 AnnData 对象\n",
    "eRegulon_auc.obs = scplus_mdata.obs.loc[eRegulon_auc.obs_names]\n",
    "\n",
    "# 计算邻居图和 UMAP\n",
    "sc.pp.neighbors(eRegulon_auc, use_rep=\"X\")\n",
    "sc.tl.umap(eRegulon_auc)\n",
    "\n",
    "# 绘制 UMAP，并根据细胞类型进行着色\n",
    "print(\"UMAP based on eRegulon activity scores:\")\n",
    "# print(f\"\\n可用于后续绘图的细胞注释信息 (e.g., in sc.pl.umap color): \\n{list(eRegulon_auc.obs.columns)}\")\n",
    "sc.pl.umap(\n",
    "    eRegulon_auc, \n",
    "    color=\"scRNA_counts:Seurat_cell_type\",\n",
    "    title=\"eRegulon Activity UMAP\",\n",
    "    frameon=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b847c3-a25c-4f00-b9b0-bb91a19cfd75",
   "metadata": {},
   "source": [
    "### **5.4. 计算并可视化 eRegulon 特异性评分 (RSS)**\n",
    "\n",
    "为了系统性地找出在每个细胞类型中特异性高活的 eRegulon，我们可以计算调控子特异性评分（Regulon Specificity Score, RSS）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b34bc09-d7eb-4a7a-9d52-12239b2134b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Calculating Regulon Specificity Scores (RSS)...\")\n",
    "rss = regulon_specificity_scores(\n",
    "    scplus_mudata=scplus_mdata,\n",
    "    variable=\"scRNA_counts:Seurat_cell_type\",\n",
    "    modalities=[\"direct_gene_based_AUC\", \"extended_gene_based_AUC\"]\n",
    ")\n",
    "\n",
    "# 绘制 RSS 热图，展示每个细胞类型中特异性最高的 top 1 eRegulons\n",
    "print(\"\\nPlotting RSS for top 3 eRegulons per cell type:\")\n",
    "plot_rss(\n",
    "    data_matrix=rss,\n",
    "    top_n=3,\n",
    "    num_columns=5  # 根据你的细胞类型数量调整布局\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d854174c-e648-4bbe-9a7c-258ea2347257",
   "metadata": {},
   "source": [
    "\n",
    "### **5.5. 在 UMAP 上可视化顶层 eRegulons**\n",
    "\n",
    "根据 RSS 的结果，我们可以选择每个细胞类型中特异性最高的几个 eRegulon，并将其活性分数直接绘制在 UMAP 上，以直观地验证其细胞类型特异性。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaa12f1-babe-4e09-8c43-e1f7e2a6e8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 从 RSS 结果中提取每个细胞类型 top 2 的 eRegulon 名称\n",
    "top_regulons_to_plot = list(set(\n",
    "    [item for sublist in [rss.loc[ct].sort_values(ascending=False).head(2).index.tolist() for ct in rss.index] for item in sublist]\n",
    "))\n",
    "\n",
    "print(f\"Visualizing activity of top eRegulons on UMAP: {top_regulons_to_plot}\")\n",
    "\n",
    "# 在 UMAP 上绘制这些 eRegulon 的 AUC 分数\n",
    "sc.pl.umap(\n",
    "    eRegulon_auc, \n",
    "    color=top_regulons_to_plot,\n",
    "    ncols=4,  # 调整布局列数\n",
    "    frameon=False,\n",
    "    cmap='viridis' # 使用 'viridis' 配色方案\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e50f3c6-b9b5-4eaa-aa3d-8252cc5740ee",
   "metadata": {},
   "source": [
    "\n",
    "### **5.6. 热图点图 (Heatmap Dotplot)**\n",
    "\n",
    "这种图可以同时展示两种信息：点的颜色代表目标基因集的富集程度（`direct_gene_based_AUC`），而点的大小代表目标区域集（增强子）的可及性富集程度（`direct_region_based_AUC`）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb429e9c-a579-43f1-9ece-86fbc6d41c9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating heatmap dotplot...\")\n",
    "heatmap_dotplot(\n",
    "    scplus_mudata=scplus_mdata,\n",
    "    color_modality=\"direct_gene_based_AUC\",\n",
    "    size_modality=\"direct_region_based_AUC\",\n",
    "    group_variable=\"scRNA_counts:Seurat_cell_type\",\n",
    "    eRegulon_metadata_key=\"direct_e_regulon_metadata\",\n",
    "    color_feature_key=\"Gene_signature_name\",\n",
    "    size_feature_key=\"Region_signature_name\",\n",
    "    feature_name_key=\"eRegulon_name\",\n",
    "    sort_data_by=\"direct_gene_based_AUC\",\n",
    "    orientation=\"horizontal\",\n",
    "    figsize=(35, 20) # 根据你的细胞类型数量调整图形大小\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51734ba6-c754-4475-bdb6-2cfaa29a1b83",
   "metadata": {},
   "source": [
    "<h1>终于跑完全部流程了，长舒一口气！Happy Coding！<h1>"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python (scenicplus)",
   "language": "python",
   "name": "scenicplus"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
